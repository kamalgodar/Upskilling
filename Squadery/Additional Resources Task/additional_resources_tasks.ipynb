{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37265f8",
   "metadata": {},
   "source": [
    "## Squadery Upskilling\n",
    "### Additional  Resources Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa0b584",
   "metadata": {},
   "source": [
    ">\n",
    "1. Define a variable my_sent to be a list of words, using the syntax my_sent = [\"My\", \"sent\"] (but with your own words, or a favorite saying). \n",
    "\n",
    "    a. Use ' '.join(my_sent) to convert this into a string\n",
    "\n",
    "    b. Use split() to split the string back into the list form you had to start with.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6943abe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is Shiva Godar'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sent = [\"My\", \"name\", \"is\", \"Shiva\", \"Godar\"]\n",
    "to_string = ' '.join(my_sent)\n",
    "to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dc1570f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'name', 'is', 'Shiva', 'Godar']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_split = to_string.split()\n",
    "string_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238b8cd",
   "metadata": {},
   "source": [
    ">\n",
    "\n",
    "2. Define several variables containing lists of words, e.g., phrase1, phrase2, and so on. Join them together in various combinations (using the plus operator) to form whole sentences. \n",
    "\n",
    "    What is the relationship between len(phrase1 + phrase2) and len(phrase1) + len(phrase2)?\n",
    "    >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5cc6a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Shiva',\n",
       " 'Godar',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Shiva',\n",
       " 'Godar',\n",
       " 'I',\n",
       " 'enjoy',\n",
       " 'travelling']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase1 = [\"My\", \"name\", \"is\", \"Shiva\", \"Godar\"]\n",
    "phrase2 = [\"I\", \"love\", \"playing\", \"football\"]\n",
    "phrase3 = [\"I\", \"enjoy\", \"travelling\"]\n",
    "\n",
    "combined_phrase = phrase1 + phrase1 + phrase3\n",
    "combined_phrase\n",
    "\n",
    "# test1 = \"Hi, this is Kamal.\"\n",
    "# test2 = 'I love football.'\n",
    "# test = test1 + test2\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "638b2c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of phrase1 is: 5\n",
      "The length of phrase2 is: 4\n",
      "The length of phrase3 is: 3\n",
      "The length of combined phrase is: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of phrase1 is:\", len(phrase1))\n",
    "print(\"The length of phrase2 is:\", len(phrase2))\n",
    "print(\"The length of phrase3 is:\", len(phrase3))\n",
    "print(\"The length of combined phrase is:\", len(combined_phrase))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d222401",
   "metadata": {},
   "source": [
    ">\n",
    "3. What is the difference between the following two lines? Which one will give a larger value? Will this be the case for other texts?\n",
    "\n",
    "    -> sorted(set(w.lower() for w in text1))\n",
    "\n",
    "    -> sorted(w.lower() for w in set(text1))\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f4ceaf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', '.', 'a', 'c', 'e', 'h', 'i', 'k', 'o', 's', 't', 'x']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'This is a test text to check.'\n",
    "# text1 = ['This', 'is', 'also', 'for', 'check']\n",
    "sorted(set(w.lower() for w in text1)) # returns the list []\n",
    "# set(w.lower() for w in text1) # returns the set {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "183005e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '.',\n",
       " 'a',\n",
       " 'c',\n",
       " 'c',\n",
       " 'e',\n",
       " 'e',\n",
       " 'e',\n",
       " 'h',\n",
       " 'h',\n",
       " 'i',\n",
       " 'i',\n",
       " 'k',\n",
       " 'o',\n",
       " 's',\n",
       " 's',\n",
       " 's',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 'x']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(w.lower() for w in text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5439ec9d",
   "metadata": {},
   "source": [
    "sorted() method returns the elements in sorted order, and lower() method lowercase all the elements\n",
    "\n",
    "set() method converts any of the iterable to sequence of iterable elements with distinct elements, while the line without the set() returns all the characters it iterates over.\n",
    "\n",
    "The line of code without set() method gives the larger value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc33e81",
   "metadata": {},
   "source": [
    ">\n",
    "4. What is the difference between the following two tests: w.isupper() and not w.islower()?\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5eb9f465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"THIS IS FOR CHECKING.\"\n",
    "x = txt.isupper()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "669abd2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = \"This is for checking.\"\n",
    "x = not txt.islower()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3aa91d",
   "metadata": {},
   "source": [
    "isupper() returns a boolean value, i.e. True if all characters are uppercase, else False.\n",
    "\n",
    "islower() returns a boolean value, i.e. True if all characters are lowercase, else False.\n",
    "\n",
    "\n",
    "not isupper() returns a boolean value, i.e. True if any character is lowercase, else False.\n",
    "\n",
    "not islower() returns a boolean value, i.e. True if any character is uppercase, else False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a222b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2454336d",
   "metadata": {},
   "source": [
    "5. What does the following Python code do? sum(len(w) for w in text1) Can you use it to work out the average word length of a text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7f67baf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'This is a test text to check.'\n",
    "x = sum(len(w) for w in text1) # returns the length of the text\n",
    "y = sum(len(w) for w in text1 if w==' ') # counting the number of spaces\n",
    "average = x//(y+1)\n",
    "average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d5670",
   "metadata": {},
   "source": [
    "The given Python code iterates over the string and returns the length of character each time which gets summed in each run and finally returns the length of the text string.\n",
    "\n",
    "The average word length in a selected text is 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f2a9d",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55361c43",
   "metadata": {},
   "source": [
    "6. Define a function percent(word, text) that calculates how often a given word occurs in a text, and expresses the result as a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e9298553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.5"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is going to be a random text. I have to repeat some words to find the percentage of occurance of word in text.\"\n",
    "\n",
    "def percent(word, text):\n",
    "    count = 0\n",
    "    text1 = text.split()\n",
    "    for w in text1:\n",
    "        if w == word:\n",
    "            count = count + 1\n",
    "    total_words = sum(len(w) for w in text if w==' ') + 1\n",
    "    percent = (count/total_words)*100\n",
    "    return percent\n",
    "    \n",
    "word = 'to'\n",
    "percent(word, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae79dab",
   "metadata": {},
   "source": [
    "Mistakes:\n",
    "\n",
    "We need to split the string into list to make a match of the word and count it.\n",
    "\n",
    "for word in text:\n",
    "    count = count + 1 #if is required and here, word doesn't mean 'to'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c3d63",
   "metadata": {},
   "source": [
    ">\n",
    "7. We have been using sets to store vocabularies. Try the following Python expression: set(sent3) < set(text1). \n",
    "\n",
    "    Experiment with this using different arguments to set(). What does it do? Can you think of a practical application for this?\n",
    "    >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8e89398e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "text1 = \"This is the\"\n",
    "text2 = \"This is\"\n",
    "text3 = \"This is\"\n",
    "\n",
    "print(set(text2) < set(text1))\n",
    "print(set(text2) < set(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "683852ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(6, 6), (1, 1), (1, 2), (2, 1)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [ (1,1), (2,1), (1,2), (6,6) ]\n",
    "x = set(X)\n",
    "print(x)\n",
    "(2,1) not in x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e92df",
   "metadata": {},
   "source": [
    "All of the standard comparisons (<, <=, >, >=, ==, !=, in , not in ) work with sets, but the interpretation of the operators is based on set theory. The comparisons determine if we have subset or superset (<=, >=) relationships between two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3b8cdba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tableau', 'SQL', 'SAS', 'R', 'Python', 'Git']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform Set into Ordered Values (List)\n",
    "dataScientist = {'Python', 'R', 'SQL', 'Git', 'Tableau', 'SAS'}\n",
    "sorted(dataScientist, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a8ca522d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Git', 'Hadoop', 'Java', 'Python', 'SQL'}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataEngineer = {'Python', 'Java', 'Scala', 'Git', 'SQL'}\n",
    "dataEngineer.add('Hadoop')\n",
    "dataEngineer.remove('Scala')\n",
    "dataEngineer # stored in alphabetic order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "76484d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Git', 'SQL', 'SAS', 'Java', 'Tableau', 'Python', 'Hadoop', 'R'}\n",
      "{'Python', 'Git', 'SQL'}\n"
     ]
    }
   ],
   "source": [
    "print(dataEngineer.union(dataScientist))\n",
    "print(dataEngineer.intersection(dataScientist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ae71f9",
   "metadata": {},
   "source": [
    ">\n",
    "8. Create a object called translate which you could look up using words in both German and Spanish in order to get corresponding words in English. What problem might arise with this approach? Can you suggest a way to avoid this problem?\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1f77ad88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting translate\n",
      "  Downloading translate-3.6.1-py2.py3-none-any.whl (12 kB)\n",
      "Collecting libretranslatepy==2.1.1\n",
      "  Downloading libretranslatepy-2.1.1-py3-none-any.whl (3.2 kB)\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.9.1-cp38-cp38-win_amd64.whl (3.6 MB)\n",
      "     ---------------------------------------- 3.6/3.6 MB 5.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\acer\\anaconda3\\envs\\nlp\\lib\\site-packages (from translate) (7.1.2)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\anaconda3\\envs\\nlp\\lib\\site-packages (from translate) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->translate) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->translate) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\acer\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->translate) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\acer\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->translate) (1.26.9)\n",
      "Installing collected packages: libretranslatepy, lxml, translate\n",
      "Successfully installed libretranslatepy-2.1.1 lxml-4.9.1 translate-3.6.1\n"
     ]
    }
   ],
   "source": [
    "! pip install translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ae351d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good morning'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from translate import Translator\n",
    "translator = Translator(from_lang=\"german\",to_lang=\"english\")\n",
    "translation = translator.translate(\"Guten Morgen\")\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "65cdc6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from translate import Translator\n",
    "translator = Translator(from_lang=\"spanish\",to_lang=\"english\")\n",
    "translation = translator.translate(\"Hola\")\n",
    "translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb59f64",
   "metadata": {},
   "source": [
    "We can do the simple text translation from one language to another using the python package called 'translate'\n",
    "\n",
    "Or, we can use GOOGLE TRANSLATE API using Python\n",
    "\n",
    "Link: https://stackabuse.com/text-translation-with-google-translate-api-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ab6663",
   "metadata": {},
   "source": [
    ">\n",
    "9. Write a program to find all words that occur at least three times in the Brown Corpus.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16e8b539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bddafe27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 6386, ',': 5188, '.': 4030, 'of': 2861, 'and': 2186, 'to': 2144, 'a': 2130, 'in': 2020, 'for': 969, 'that': 829, ...})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_text = brown.words(categories='news')\n",
    "fdist = nltk.FreqDist(w.lower() for w in news_text)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f504bada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words that appeared more than 500 times in Brown Corpus in news category are:\n",
      "{'the': 806, '``': 732, \"''\": 702, '.': 4030, ',': 5188}\n"
     ]
    }
   ],
   "source": [
    "newDict = dict()\n",
    "\n",
    "for (key, value) in fdist.items():\n",
    "    if value > 500:\n",
    "       newDict[key] = value\n",
    "print(\"Words that appeared more than 500 times in Brown Corpus in news category are:\")\n",
    "print(newDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25c0c46",
   "metadata": {},
   "source": [
    ">\n",
    "10. Write a function that finds the 50 most frequently occurring words of a text that are not stopwords\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea23caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_text = brown.words(categories='news')\n",
    "news_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6069b6ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0246ff30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100554\n",
      "66493\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'investigation', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'evidence', \"''\", 'irregularities', 'took', 'place', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "words_without_stopwords = []\n",
    "for w in news_text:\n",
    "    if w not in stop_words:\n",
    "        words_without_stopwords.append(w)\n",
    "        \n",
    "print(len(news_text))\n",
    "print(len(words_without_stopwords))\n",
    "print(words_without_stopwords[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7bed0ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of top 50 frequently occuring words in a text that are not stopwords are:\n",
      "{'the': 806, 'fulton': 14, 'county': 61, 'grand': 19, 'jury': 46, 'said': 406, 'friday': 41, 'investigation': 11, \"atlanta's\": 4, 'recent': 20, 'primary': 17, 'election': 41, 'produced': 6, '``': 732, 'evidence': 17, \"''\": 702, 'irregularities': 3, 'took': 47, 'place': 33, '.': 4030, 'term-end': 1, 'presentments': 1, 'city': 93, 'executive': 18, 'committee': 75, ',': 5188, 'over-all': 2, 'charge': 18, 'deserves': 3, 'praise': 2, 'thanks': 6, 'atlanta': 14, 'manner': 7, 'conducted': 8, 'september-october': 1, 'term': 13, 'charged': 12, 'superior': 7, 'court': 55, 'judge': 39, 'durwood': 1, 'pye': 1, 'investigate': 3, 'reports': 13, 'possible': 29, 'hard-fought': 1, 'mayor-nominate': 1, 'ivan': 2, 'allen': 7, 'jr.': 46}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(w.lower() for w in words_without_stopwords)\n",
    "newDict = dict()\n",
    "count = 0\n",
    "\n",
    "for (key, value) in fdist.items():     # why the order of items in fdist and newDict not the same?\n",
    "    if count < 50:\n",
    "        newDict[key] = value\n",
    "    count = count + 1\n",
    "print(\"List of top 50 frequently occuring words in a text that are not stopwords are:\")\n",
    "print(newDict)\n",
    "len(newDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e985a4",
   "metadata": {},
   "source": [
    ">\n",
    "11. Write a program to print the 50 most frequent bigrams (pairs of adjacent words) of a text, omitting bigrams that contain stopwords\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5d5cdda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'Fulton'),\n",
       " ('Fulton', 'County'),\n",
       " ('County', 'Grand'),\n",
       " ('Grand', 'Jury'),\n",
       " ('Jury', 'said'),\n",
       " ('said', 'Friday'),\n",
       " ('Friday', 'an'),\n",
       " ('an', 'investigation'),\n",
       " ('investigation', 'of'),\n",
       " ('of', \"Atlanta's\"),\n",
       " (\"Atlanta's\", 'recent'),\n",
       " ('recent', 'primary'),\n",
       " ('primary', 'election'),\n",
       " ('election', 'produced'),\n",
       " ('produced', '``'),\n",
       " ('``', 'no'),\n",
       " ('no', 'evidence'),\n",
       " ('evidence', \"''\"),\n",
       " (\"''\", 'that'),\n",
       " ('that', 'any')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_text = brown.words(categories='news')\n",
    "bi_tokens = list(nltk.bigrams(news_text))\n",
    "bi_tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fead4f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100554\n",
      "100553\n",
      "43268\n",
      "[('The', 'Fulton'), ('Fulton', 'County'), ('County', 'Grand'), ('Grand', 'Jury'), ('Jury', 'said'), ('said', 'Friday'), (\"Atlanta's\", 'recent'), ('recent', 'primary'), ('primary', 'election'), ('election', 'produced'), ('produced', '``'), ('evidence', \"''\"), ('irregularities', 'took'), ('took', 'place'), ('place', '.'), ('.', 'The'), ('The', 'jury'), ('term-end', 'presentments'), ('City', 'Executive'), ('Executive', 'Committee'), ('Committee', ','), ('over-all', 'charge'), ('election', ','), (',', '``'), ('``', 'deserves'), ('Atlanta', \"''\"), ('conducted', '.'), ('.', 'The'), ('The', 'September-October'), ('September-October', 'term')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "words_without_stopwords1 = []\n",
    "for w in bi_tokens:\n",
    "    for i in range(0,2):\n",
    "        if w[i] in stop_words:                   # here...\n",
    "            break\n",
    "        if i==1:                                 # as for bigram the last index is 1\n",
    "            words_without_stopwords1.append(w)\n",
    "            \n",
    "print(len(news_text))\n",
    "print(len(bi_tokens))\n",
    "print(len(words_without_stopwords1))\n",
    "print(words_without_stopwords1[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c6132783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of top 50 most frequent bigrams of text omitting bigrams that contains stopwords are:\n",
      "{('The', 'Fulton'): 1, ('Fulton', 'County'): 6, ('County', 'Grand'): 1, ('Grand', 'Jury'): 2, ('Jury', 'said'): 1, ('said', 'Friday'): 4, (\"Atlanta's\", 'recent'): 1, ('recent', 'primary'): 1, ('primary', 'election'): 2, ('election', 'produced'): 1, ('produced', '``'): 1, ('evidence', \"''\"): 2, ('irregularities', 'took'): 1, ('took', 'place'): 4, ('place', '.'): 9, ('.', 'The'): 659, ('The', 'jury'): 9, ('term-end', 'presentments'): 1, ('City', 'Executive'): 1, ('Executive', 'Committee'): 2, ('Committee', ','): 7, ('over-all', 'charge'): 1, ('election', ','): 4, (',', '``'): 95, ('``', 'deserves'): 1, ('Atlanta', \"''\"): 1, ('conducted', '.'): 1, ('The', 'September-October'): 1, ('September-October', 'term'): 1, ('term', 'jury'): 1, ('Fulton', 'Superior'): 2, ('Superior', 'Court'): 5, ('Court', 'Judge'): 1, ('Judge', 'Durwood'): 1, ('Durwood', 'Pye'): 1, ('investigate', 'reports'): 1, ('possible', '``'): 1, ('``', 'irregularities'): 1, ('irregularities', \"''\"): 1, ('hard-fought', 'primary'): 1, ('Mayor-nominate', 'Ivan'): 1, ('Ivan', 'Allen'): 2, ('Allen', 'Jr.'): 2, ('Jr.', '.'): 3, ('.', '``'): 237, ('``', 'Only'): 1, ('relative', 'handful'): 1, ('received', \"''\"): 1, (\"''\", ','): 198, ('jury', 'said'): 7}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(w for w in words_without_stopwords1)\n",
    "newDict = dict()\n",
    "count = 0\n",
    "\n",
    "for (key, value) in fdist.items():\n",
    "    if count < 50:\n",
    "        newDict[key] = value\n",
    "    count = count + 1\n",
    "print(\"List of top 50 most frequent bigrams of text omitting bigrams that contains stopwords are:\")\n",
    "print(newDict)\n",
    "len(newDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eb4539",
   "metadata": {},
   "source": [
    ">\n",
    "12. Write a program to create a table of word frequencies by genre, like the one given in 1 for modals. Choose your own words and try to find words whose presence (or absence) is typical of a genre. Discuss your findings\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "daae8d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      joke      death     temple     travel technology       love \n",
      "           news          0         10          0          2          3          3 \n",
      "       religion          0         41          1          2          0         13 \n",
      "        hobbies          0          3          2          8          5          6 \n",
      "science_fiction          2          2          0          0          0          3 \n",
      "        romance          2         12          8          2          0         32 \n",
      "          humor          1          1          0          0          0          4 \n"
     ]
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "        (genre, word)\n",
    "        for genre in brown.categories()\n",
    "        for word in brown.words(categories=genre))\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "words = ['joke', 'death', 'temple', 'travel', 'technology', 'love']\n",
    "cfd.tabulate(conditions=genres, samples=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c6edb",
   "metadata": {},
   "source": [
    "The above table shows the number of presence of the selected words (joke, death, temple, travel, technology, love) in the different categories of text (news, religion, hobbies, science_fiction, romance, humor) in brown corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1f7c98",
   "metadata": {},
   "source": [
    ">\n",
    "13. Zipf's Law: Let f(w) be the frequency of a word w in free text. Suppose that all the words of a text are ranked according to their frequency, with the most frequent word first. Zipf's law states that the frequency of a word type is inversely proportional to its rank (i.e. f × r = k, for some constant k). For example, the 50th most common word type should occur three times as frequently as the 150th most common word type.\n",
    " \n",
    "    a. Write a function to process a large text and plot word frequency against word rank using pylab.plot. Do you confirm Zipf's law? (Hint: it helps to use a logarithmic scale). What is going on at the extreme ends of the plotted line?\n",
    "\n",
    "    b. Generate random text, e.g., using random.choice(\"abcdefg \"), taking care to include the space character. You will need to import random first. Use the string concatenation operator to accumulate characters into a (very) long string. Then tokenize this string, and generate the Zipf plot as before, and compare the two plots. What do you make of Zipf's Law in light of this?\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f4e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3187d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28de2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de63627c",
   "metadata": {},
   "source": [
    ">\n",
    "14. Define a function find_language() that takes a string as its argument, and returns a list of languages that have that string as a word. Use the udhr corpus and limit your searches to files in the Latin-1 encoding.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a37b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a337fe12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a469ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "355f1751",
   "metadata": {},
   "source": [
    ">\n",
    "15. Define a string s = ‘colorless’. Write a python statement that changes this to “colorless” using only the slice and concatenation operations.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdda1c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370c387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31519053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08de7a9b",
   "metadata": {},
   "source": [
    ">\n",
    "16. We saw how we can generate an IndexError by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38431a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5126a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b87e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
