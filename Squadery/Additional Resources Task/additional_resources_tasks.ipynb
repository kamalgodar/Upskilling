{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37265f8",
   "metadata": {},
   "source": [
    "## Squadery Upskilling\n",
    "### Additional  Resources Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa0b584",
   "metadata": {},
   "source": [
    "\n",
    "**1. Define a variable my_sent to be a list of words, using the syntax my_sent = [\"My\", \"sent\"] (but with your own words, or a favorite saying). \\\n",
    "    a. Use ' '.join(my_sent) to convert this into a string \\\n",
    "    b. Use split() to split the string back into the list form you had to start with.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6943abe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is Shiva Godar'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sent = [\"My\", \"name\", \"is\", \"Shiva\", \"Godar\"]\n",
    "to_string = ' '.join(my_sent)\n",
    "to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8dc1570f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My', 'name', 'is', 'Shiva', 'Godar']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_split = to_string.split()\n",
    "string_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e4210c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238b8cd",
   "metadata": {},
   "source": [
    "**2. Define several variables containing lists of words, e.g., phrase1, phrase2, and so on. Join them together in various combinations (using the plus operator) to form whole sentences.\\\n",
    "    What is the relationship between len(phrase1 + phrase2) and len(phrase1) + len(phrase2)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b5cc6a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Shiva',\n",
       " 'Godar',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Shiva',\n",
       " 'Godar',\n",
       " 'I',\n",
       " 'enjoy',\n",
       " 'travelling']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase1 = [\"My\", \"name\", \"is\", \"Shiva\", \"Godar\"]\n",
    "phrase2 = [\"I\", \"love\", \"playing\", \"football\"]\n",
    "phrase3 = [\"I\", \"enjoy\", \"travelling\"]\n",
    "\n",
    "combined_phrase = phrase1 + phrase1 + phrase3\n",
    "combined_phrase\n",
    "\n",
    "# test1 = \"Hi, this is Kamal.\"\n",
    "# test2 = 'I love football.'\n",
    "# test = test1 + test2\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "638b2c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of phrase1 is: 5\n",
      "The length of phrase2 is: 4\n",
      "The length of phrase3 is: 3\n",
      "The length of combined phrase is: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of phrase1 is:\", len(phrase1))\n",
    "print(\"The length of phrase2 is:\", len(phrase2))\n",
    "print(\"The length of phrase3 is:\", len(phrase3))\n",
    "print(\"The length of combined phrase is:\", len(combined_phrase))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f594f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d222401",
   "metadata": {},
   "source": [
    "**3. What is the difference between the following two lines? Which one will give a larger value? Will this be the case for other texts? \\\n",
    "    -> sorted(set(w.lower() for w in text1)) \\\n",
    "    -> sorted(w.lower() for w in set(text1))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f4ceaf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', '.', 'a', 'c', 'e', 'h', 'i', 'k', 'o', 's', 't', 'x']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'This is a test text to check.'\n",
    "# text1 = ['This', 'is', 'also', 'for', 'check']\n",
    "sorted(set(w.lower() for w in text1)) # returns the list []\n",
    "# set(w.lower() for w in text1) # returns the set {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "183005e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " '.',\n",
       " 'a',\n",
       " 'c',\n",
       " 'c',\n",
       " 'e',\n",
       " 'e',\n",
       " 'e',\n",
       " 'h',\n",
       " 'h',\n",
       " 'i',\n",
       " 'i',\n",
       " 'k',\n",
       " 'o',\n",
       " 's',\n",
       " 's',\n",
       " 's',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 't',\n",
       " 'x']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(w.lower() for w in text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5439ec9d",
   "metadata": {},
   "source": [
    "sorted() method returns the elements in sorted order, and lower() method lowercase all the elements\n",
    "\n",
    "set() method converts any of the iterable to sequence of iterable elements with distinct elements, while the line without the set() returns all the characters it iterates over.\n",
    "\n",
    "The line of code without set() method gives the larger value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0e9dea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc33e81",
   "metadata": {},
   "source": [
    "**4. What is the difference between the following two tests: w.isupper() and not w.islower()?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5eb9f465",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'txt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m text1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTHIS IS FOR CHECKING.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtxt\u001b[49m\u001b[38;5;241m.\u001b[39misupper()\n\u001b[0;32m      3\u001b[0m x\n",
      "\u001b[1;31mNameError\u001b[0m: name 'txt' is not defined"
     ]
    }
   ],
   "source": [
    "text1 = \"THIS IS FOR CHECKING.\"\n",
    "x = txt.isupper()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "669abd2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'txt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m text2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is for checking.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtxt\u001b[49m\u001b[38;5;241m.\u001b[39mislower()\n\u001b[0;32m      3\u001b[0m x\n",
      "\u001b[1;31mNameError\u001b[0m: name 'txt' is not defined"
     ]
    }
   ],
   "source": [
    "text2 = \"This is for checking.\"\n",
    "x = not txt.islower()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3aa91d",
   "metadata": {},
   "source": [
    "isupper() returns a boolean value, i.e. True if all characters are uppercase, else False.\n",
    "\n",
    "islower() returns a boolean value, i.e. True if all characters are lowercase, else False.\n",
    "\n",
    "\n",
    "not isupper() returns a boolean value, i.e. True if any character is lowercase, else False.\n",
    "\n",
    "not islower() returns a boolean value, i.e. True if any character is uppercase, else False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a222b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2454336d",
   "metadata": {},
   "source": [
    "**5. What does the following Python code do? sum(len(w) for w in text1) Can you use it to work out the average word length of a text?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f67baf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'This is a test text to check.'\n",
    "x = sum(len(w) for w in text1) # returns the length of the text\n",
    "y = sum(len(w) for w in text1 if w==' ') # counting the number of spaces\n",
    "average = x//(y+1)\n",
    "average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d5670",
   "metadata": {},
   "source": [
    "The given Python code iterates over the string and returns the length of character each time which gets summed in each run and finally returns the length of the text string.\n",
    "\n",
    "The average word length in a selected text is 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f2a9d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55361c43",
   "metadata": {},
   "source": [
    "**6. Define a function percent(word, text) that calculates how often a given word occurs in a text, and expresses the result as a percentage.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e9298553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.5"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is going to be a random text. I have to repeat some words to find the percentage of occurance of word in text.\"\n",
    "\n",
    "def percent(word, text):\n",
    "    count = 0\n",
    "    text1 = text.split()\n",
    "    for w in text1:\n",
    "        if w == word:\n",
    "            count = count + 1\n",
    "    total_words = sum(len(w) for w in text if w==' ') + 1\n",
    "    percent = (count/total_words)*100\n",
    "    return percent\n",
    "    \n",
    "word = 'to'\n",
    "percent(word, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae79dab",
   "metadata": {},
   "source": [
    "Mistakes:\n",
    "\n",
    "We need to split the string into list to make a match of the word and count it.\n",
    "\n",
    "for word in text:\n",
    "    count = count + 1 #if is required and here, word doesn't mean 'to'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307d9a26",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c3d63",
   "metadata": {},
   "source": [
    "**7. We have been using sets to store vocabularies. Try the following Python expression: set(sent3) < set(text1). \\\n",
    "    Experiment with this using different arguments to set(). What does it do? Can you think of a practical application for this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e89398e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "text1 = \"This is the\"\n",
    "text2 = \"This is\"\n",
    "text3 = \"This is\"\n",
    "\n",
    "print(set(text2) < set(text1))\n",
    "print(set(text2) < set(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "683852ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(6, 6), (1, 1), (1, 2), (2, 1)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [ (1,1), (2,1), (1,2), (6,6) ]\n",
    "x = set(X)\n",
    "print(x)\n",
    "(2,1) not in x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e92df",
   "metadata": {},
   "source": [
    "All of the standard comparisons (<, <=, >, >=, ==, !=, in , not in ) work with sets, but the interpretation of the operators is based on set theory. The comparisons determine if we have subset or superset (<=, >=) relationships between two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3b8cdba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tableau', 'SQL', 'SAS', 'R', 'Python', 'Git']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform Set into Ordered Values (List)\n",
    "dataScientist = {'Python', 'R', 'SQL', 'Git', 'Tableau', 'SAS'}\n",
    "sorted(dataScientist, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a8ca522d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Git', 'Hadoop', 'Java', 'Python', 'SQL'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataEngineer = {'Python', 'Java', 'Scala', 'Git', 'SQL'}\n",
    "dataEngineer.add('Hadoop')\n",
    "dataEngineer.remove('Scala')\n",
    "dataEngineer # stored in alphabetic order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "76484d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Python', 'Java', 'Tableau', 'SAS', 'R', 'SQL', 'Hadoop', 'Git'}\n",
      "{'SQL', 'Git', 'Python'}\n"
     ]
    }
   ],
   "source": [
    "print(dataEngineer.union(dataScientist))\n",
    "print(dataEngineer.intersection(dataScientist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf177cf9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ae71f9",
   "metadata": {},
   "source": [
    "**8. Create a object called translate which you could look up using words in both German and Spanish in order to get corresponding words in English. What problem might arise with this approach? Can you suggest a way to avoid this problem?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1f77ad88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: translate in c:\\users\\acer\\anaconda3\\envs\\nlp\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\acer\\anaconda3\\envs\\nlp\\lib\\site-packages (from translate) (4.9.1)\n",
      "Requirement already satisfied: libretranslatepy==2.1.1 in c:\\users\\acer\\anaconda3\\envs\\nlp\\lib\\site-packages (from translate) (2.1.1)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\anaconda3\\envs\\nlp\\lib\\site-packages (from translate) (2.28.1)\n",
      "Requirement already satisfied: click in c:\\users\\acer\\anaconda3\\envs\\nlp\\lib\\site-packages (from translate) (7.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\acer\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->translate) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\acer\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->translate) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\acer\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->translate) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\acer\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->translate) (2022.6.15)\n"
     ]
    }
   ],
   "source": [
    "! pip install translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5ae351d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good morning'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from translate import Translator\n",
    "translator = Translator(from_lang=\"german\",to_lang=\"english\")\n",
    "translation = translator.translate(\"Guten Morgen\")\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "65cdc6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from translate import Translator\n",
    "translator = Translator(from_lang=\"spanish\",to_lang=\"english\")\n",
    "translation = translator.translate(\"Hola\")\n",
    "translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb59f64",
   "metadata": {},
   "source": [
    "We can do the simple text translation from one language to another using the python package called 'translate'\n",
    "\n",
    "Or, we can use GOOGLE TRANSLATE API using Python\n",
    "\n",
    "Link: https://stackabuse.com/text-translation-with-google-translate-api-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d2623",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ab6663",
   "metadata": {},
   "source": [
    "**9. Write a program to find all words that occur at least three times in the Brown Corpus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "16e8b539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bddafe27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 6386, ',': 5188, '.': 4030, 'of': 2861, 'and': 2186, 'to': 2144, 'a': 2130, 'in': 2020, 'for': 969, 'that': 829, ...})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_text = brown.words(categories='news')\n",
    "fdist = nltk.FreqDist(w.lower() for w in news_text)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f504bada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words that appeared more than 500 times in Brown Corpus in news category are:\n",
      "{'the': 6386, 'of': 2861, '``': 732, \"''\": 702, 'that': 829, '.': 4030, 'in': 2020, ',': 5188, 'and': 2186, 'for': 969, 'was': 717, 'by': 504, 'to': 2144, 'a': 2130, 'on': 691, 'be': 526, 'is': 733, 'as': 517, 'at': 636, 'with': 567, 'he': 642}\n"
     ]
    }
   ],
   "source": [
    "newDict = dict()\n",
    "\n",
    "for (key, value) in fdist.items():\n",
    "    if value > 500:\n",
    "       newDict[key] = value\n",
    "print(\"Words that appeared more than 500 times in Brown Corpus in news category are:\")\n",
    "print(newDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b6cc2d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25c0c46",
   "metadata": {},
   "source": [
    "**10. Write a function that finds the 50 most frequently occurring words of a text that are not stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dea23caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_text = brown.words(categories='news')\n",
    "news_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6069b6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0246ff30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100554\n",
      "66493\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'investigation', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'evidence', \"''\", 'irregularities', 'took', 'place', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "words_without_stopwords = []\n",
    "for w in news_text:\n",
    "    if w not in stop_words:\n",
    "        words_without_stopwords.append(w)\n",
    "        \n",
    "print(len(news_text))\n",
    "print(len(words_without_stopwords))\n",
    "print(words_without_stopwords[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7bed0ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of top 50 frequently occuring words in a text that are not stopwords are:\n",
      "{'the': 806, 'fulton': 14, 'county': 61, 'grand': 19, 'jury': 46, 'said': 406, 'friday': 41, 'investigation': 11, \"atlanta's\": 4, 'recent': 20, 'primary': 17, 'election': 41, 'produced': 6, '``': 732, 'evidence': 17, \"''\": 702, 'irregularities': 3, 'took': 47, 'place': 33, '.': 4030, 'term-end': 1, 'presentments': 1, 'city': 93, 'executive': 18, 'committee': 75, ',': 5188, 'over-all': 2, 'charge': 18, 'deserves': 3, 'praise': 2, 'thanks': 6, 'atlanta': 14, 'manner': 7, 'conducted': 8, 'september-october': 1, 'term': 13, 'charged': 12, 'superior': 7, 'court': 55, 'judge': 39, 'durwood': 1, 'pye': 1, 'investigate': 3, 'reports': 13, 'possible': 29, 'hard-fought': 1, 'mayor-nominate': 1, 'ivan': 2, 'allen': 7, 'jr.': 46}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(w.lower() for w in words_without_stopwords)\n",
    "newDict = dict()\n",
    "count = 0\n",
    "\n",
    "for (key, value) in fdist.items():     # why the order of items in fdist and newDict not the same?\n",
    "    if count < 50:\n",
    "        newDict[key] = value\n",
    "    count = count + 1\n",
    "print(\"List of top 50 frequently occuring words in a text that are not stopwords are:\")\n",
    "print(newDict)\n",
    "len(newDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c89618",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e985a4",
   "metadata": {},
   "source": [
    "**11. Write a program to print the 50 most frequent bigrams (pairs of adjacent words) of a text, omitting bigrams that contain stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e5d5cdda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'Fulton'),\n",
       " ('Fulton', 'County'),\n",
       " ('County', 'Grand'),\n",
       " ('Grand', 'Jury'),\n",
       " ('Jury', 'said'),\n",
       " ('said', 'Friday'),\n",
       " ('Friday', 'an'),\n",
       " ('an', 'investigation'),\n",
       " ('investigation', 'of'),\n",
       " ('of', \"Atlanta's\"),\n",
       " (\"Atlanta's\", 'recent'),\n",
       " ('recent', 'primary'),\n",
       " ('primary', 'election'),\n",
       " ('election', 'produced'),\n",
       " ('produced', '``'),\n",
       " ('``', 'no'),\n",
       " ('no', 'evidence'),\n",
       " ('evidence', \"''\"),\n",
       " (\"''\", 'that'),\n",
       " ('that', 'any')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_text = brown.words(categories='news')\n",
    "bi_tokens = list(nltk.bigrams(news_text))\n",
    "bi_tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fead4f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100554\n",
      "100553\n",
      "43268\n",
      "[('The', 'Fulton'), ('Fulton', 'County'), ('County', 'Grand'), ('Grand', 'Jury'), ('Jury', 'said'), ('said', 'Friday'), (\"Atlanta's\", 'recent'), ('recent', 'primary'), ('primary', 'election'), ('election', 'produced'), ('produced', '``'), ('evidence', \"''\"), ('irregularities', 'took'), ('took', 'place'), ('place', '.'), ('.', 'The'), ('The', 'jury'), ('term-end', 'presentments'), ('City', 'Executive'), ('Executive', 'Committee'), ('Committee', ','), ('over-all', 'charge'), ('election', ','), (',', '``'), ('``', 'deserves'), ('Atlanta', \"''\"), ('conducted', '.'), ('.', 'The'), ('The', 'September-October'), ('September-October', 'term')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "words_without_stopwords1 = []\n",
    "for w in bi_tokens:\n",
    "    for i in range(0,2):\n",
    "        if w[i] in stop_words:                   # here...\n",
    "            break\n",
    "        if i==1:                                 # as for bigram the last index is 1\n",
    "            words_without_stopwords1.append(w)\n",
    "            \n",
    "print(len(news_text))\n",
    "print(len(bi_tokens))\n",
    "print(len(words_without_stopwords1))\n",
    "print(words_without_stopwords1[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c6132783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of top 50 most frequent bigrams of text omitting bigrams that contains stopwords are:\n",
      "{('The', 'Fulton'): 1, ('Fulton', 'County'): 6, ('County', 'Grand'): 1, ('Grand', 'Jury'): 2, ('Jury', 'said'): 1, ('said', 'Friday'): 4, (\"Atlanta's\", 'recent'): 1, ('recent', 'primary'): 1, ('primary', 'election'): 2, ('election', 'produced'): 1, ('produced', '``'): 1, ('evidence', \"''\"): 2, ('irregularities', 'took'): 1, ('took', 'place'): 4, ('place', '.'): 9, ('.', 'The'): 659, ('The', 'jury'): 9, ('term-end', 'presentments'): 1, ('City', 'Executive'): 1, ('Executive', 'Committee'): 2, ('Committee', ','): 7, ('over-all', 'charge'): 1, ('election', ','): 4, (',', '``'): 95, ('``', 'deserves'): 1, ('Atlanta', \"''\"): 1, ('conducted', '.'): 1, ('The', 'September-October'): 1, ('September-October', 'term'): 1, ('term', 'jury'): 1, ('Fulton', 'Superior'): 2, ('Superior', 'Court'): 5, ('Court', 'Judge'): 1, ('Judge', 'Durwood'): 1, ('Durwood', 'Pye'): 1, ('investigate', 'reports'): 1, ('possible', '``'): 1, ('``', 'irregularities'): 1, ('irregularities', \"''\"): 1, ('hard-fought', 'primary'): 1, ('Mayor-nominate', 'Ivan'): 1, ('Ivan', 'Allen'): 2, ('Allen', 'Jr.'): 2, ('Jr.', '.'): 3, ('.', '``'): 237, ('``', 'Only'): 1, ('relative', 'handful'): 1, ('received', \"''\"): 1, (\"''\", ','): 198, ('jury', 'said'): 7}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(w for w in words_without_stopwords1)\n",
    "newDict = dict()\n",
    "count = 0\n",
    "\n",
    "for (key, value) in fdist.items():\n",
    "    if count < 50:\n",
    "        newDict[key] = value\n",
    "    count = count + 1\n",
    "print(\"List of top 50 most frequent bigrams of text omitting bigrams that contains stopwords are:\")\n",
    "print(newDict)\n",
    "len(newDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6912d66",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eb4539",
   "metadata": {},
   "source": [
    "**12. Write a program to create a table of word frequencies by genre, like the one given in 1 for modals. Choose your own words and try to find words whose presence (or absence) is typical of a genre. Discuss your findings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "daae8d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      joke      death     temple     travel technology       love \n",
      "           news          0         10          0          2          3          3 \n",
      "       religion          0         41          1          2          0         13 \n",
      "        hobbies          0          3          2          8          5          6 \n",
      "science_fiction          2          2          0          0          0          3 \n",
      "        romance          2         12          8          2          0         32 \n",
      "          humor          1          1          0          0          0          4 \n"
     ]
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "        (genre, word)\n",
    "        for genre in brown.categories()\n",
    "        for word in brown.words(categories=genre))\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "words = ['joke', 'death', 'temple', 'travel', 'technology', 'love']\n",
    "cfd.tabulate(conditions=genres, samples=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c6edb",
   "metadata": {},
   "source": [
    "The above table shows the number of presence of the selected words (joke, death, temple, travel, technology, love) in the different categories of text (news, religion, hobbies, science_fiction, romance, humor) in brown corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a97849",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1f7c98",
   "metadata": {},
   "source": [
    "**13. Zipf's Law: Let f(w) be the frequency of a word w in free text. Suppose that all the words of a text are ranked according to their frequency, with the most frequent word first. Zipf's law states that the frequency of a word type is inversely proportional to its rank (i.e. f × r = k, for some constant k). For example, the 50th most common word type should occur three times as frequently as the 150th most common word type.\\\n",
    "\\\n",
    "    a. Write a function to process a large text and plot word frequency against word rank using pylab.plot. Do you confirm Zipf's law? (Hint: it helps to use a logarithmic scale). What is going on at the extreme ends of the plotted line?\\\n",
    "    \\\n",
    "    b. Generate random text, e.g., using random.choice(\"abcdefg \"), taking care to include the space character. You will need to import random first. Use the string concatenation operator to accumulate characters into a (very) long string. Then tokenize this string, and generate the Zipf plot as before, and compare the two plots. What do you make of Zipf's Law in light of this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4f6f4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, pylab, matplotlib, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "04cf640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_rank(text):\n",
    "    fdist = nltk.FreqDist([w.lower() for w in text])\n",
    "    freq = []\n",
    "    rank = []\n",
    "    n = 1\n",
    "    \n",
    "    for (key, value) in fdist.items():\n",
    "        freq.append(value)\n",
    "        rank.append(n)\n",
    "        n = n + 1\n",
    "        \n",
    "    pylab.plot(rank, freq)\n",
    "    return pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9ac87676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_text = brown.words(categories='news')\n",
    "news_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6fe49078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq_rank(news_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1312f42e",
   "metadata": {},
   "source": [
    "Yes, it follows the Zipf's law. The count of the number of words according to the rank is decreasing i.e. the repitition of given words is inversely proportional to its position in the frequency table.\n",
    "\n",
    "At the extreme ends of the plot, the occurance of words are seen to appear not more than one or two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d2a37",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de63627c",
   "metadata": {},
   "source": [
    "**14. Define a function find_language() that takes a string as its argument, and returns a list of languages that have that string as a word. Use the udhr corpus and limit your searches to files in the Latin-1 encoding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a337fe12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abkhaz-Cyrillic+Abkh',\n",
       " 'Abkhaz-UTF8',\n",
       " 'Achehnese-Latin1',\n",
       " 'Achuar-Shiwiar-Latin1',\n",
       " 'Adja-UTF8',\n",
       " 'Afaan_Oromo_Oromiffa-Latin1',\n",
       " 'Afrikaans-Latin1',\n",
       " 'Aguaruna-Latin1',\n",
       " 'Akuapem_Twi-UTF8',\n",
       " 'Albanian_Shqip-Latin1',\n",
       " 'Amahuaca',\n",
       " 'Amahuaca-Latin1',\n",
       " 'Amarakaeri-Latin1',\n",
       " 'Amuesha-Yanesha-UTF8',\n",
       " 'Arabela-Latin1',\n",
       " 'Arabic_Alarabia-Arabic',\n",
       " 'Asante-UTF8',\n",
       " 'Ashaninca-Latin1',\n",
       " 'Asheninca-Latin1',\n",
       " 'Asturian_Bable-Latin1']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import udhr\n",
    "nltk.corpus.udhr.fileids()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "45a469ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PEUNYATAANUMUM', 'TEUNTANG', 'HAK', '-', 'HAK', ...]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.udhr.words('Achehnese-Latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "11a37b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Achehnese-Latin1']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_language(string):\n",
    "    languages = []\n",
    "    target_languages = []\n",
    "\n",
    "    for name in nltk.corpus.udhr.fileids():\n",
    "        if 'Latin1' in name:\n",
    "            languages.append(name)\n",
    "\n",
    "    for lang in languages:\n",
    "        if string in nltk.corpus.udhr.words(lang):\n",
    "            target_languages.append(lang)\n",
    "\n",
    "    return target_languages\n",
    "\n",
    "text1 = 'TEUNTANG'\n",
    "find_language(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2682c88",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f1751",
   "metadata": {},
   "source": [
    "**15. Define a string s = ‘colorless’. Write a python statement that changes this to “colorless” using only the slice and concatenation operations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bdda1c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'colorless'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'colorless'\n",
    "s_new = s[0:4] + s[4:]\n",
    "s_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b35d06f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08de7a9b",
   "metadata": {},
   "source": [
    "**16. We saw how we can generate an IndexError by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23717806",
   "metadata": {},
   "source": [
    "Yes. Both negative or positive indexing beyond the length of the string causes an IndexError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d5126a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [94]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am happy.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(string[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m11\u001b[39m]) \u001b[38;5;66;03m# negative indexing\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstring\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "string = \"I am happy.\"\n",
    "print(string[-11]) # negative indexing\n",
    "print(string[-12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423a4188",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e467c7d5",
   "metadata": {},
   "source": [
    "#### 17. Describe the class of strings matched by the following regular expressions. Test your answer using nltk.re_show(). \n",
    "    a. [a-zA-Z]+ \n",
    "    b. [A-Z][a-z]* \n",
    "    c. p[aeiou]{,2}t \n",
    "    d. \\d+(\\.\\d+)? \n",
    "    e. ([^aeiou][aeiou][^aeiou])* \n",
    "    f. \\w+|[^\\w\\s]+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec21da94",
   "metadata": {},
   "source": [
    "a. [a-zA-Z]+ \\\n",
    "words with (one or more) characters either Uppercase of Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d121a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7dc55ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{I} {am} {kamal} {big}985{this}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'[a-zA-Z]+', 'I am kamal big985this ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5c9ae2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['October', 'Hello', 'okay']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.re.findall(r'[a-zA-Z]+', 'October2009 Hello okay')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695e481",
   "metadata": {},
   "source": [
    "b. [A-Z][a-z]* \\\n",
    "words starting with first character Uppercase (with lowecase occuring 0 or more times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "790a283b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{I} am kamal big985this {This}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'[A-Z][a-z]*', 'I am kamal big985this This ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e769b6e",
   "metadata": {},
   "source": [
    "c. p[aeiou]{,2}t \\\n",
    "word starting with p, ending with t and 0 to 2 vowels in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dd2cb87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This {pet} parrot {poet} port Pot {pot}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'p[aeiou]{,2}t', 'This pet parrot poet port Pot pot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2ac7d",
   "metadata": {},
   "source": [
    "d. \\d+(\\.\\d+)? \\\n",
    "Real number (interger or flot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "77bf9b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{98.56} {566} this is {5.55}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'\\d+(.\\d+)?', '98.56 566 this is 5.55')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ccf219",
   "metadata": {},
   "source": [
    "e. ([^aeiou][aeiou][^aeiou])* \\\n",
    "set of characters with the combination of Consonant-Vowel-Consonant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c847f3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T{his} {pet} {parrot} poet {por}t {Pot} {pot}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'([^aeiou][aeiou][^aeiou])+', 'This pet parrot poet port Pot pot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa90d2",
   "metadata": {},
   "source": [
    "f. \\w+|[^\\w\\s]+ \\\n",
    "alphanumeric characters or non-whitespace characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "be767915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{This89}{@}{this} {pet} {98_parr}{\\}{ot} {poet} {756} {port} {Pot} {pot}\n"
     ]
    }
   ],
   "source": [
    "nltk.re_show(r'\\w+|[^\\w\\s]+', 'This89@this pet 98_parr\\ot poet 756 port Pot pot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e711a017",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1dc13b",
   "metadata": {},
   "source": [
    "**18. The IOB format categorizes tagged tokens as I, O, and B. Why are three tags necessary? What problem would be caused if we used I and O tags exclusively?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6f217f",
   "metadata": {},
   "source": [
    "IOB format is a common tagging format for tagging tokens in a chunking task. These tags are similar to part-of-speech tags but can denote the inside, outside, and beginning of a chunk.\n",
    "\n",
    "IOB stands for (I)inside (O)outside (B)begining tags. \\\n",
    "B indicates the begining of chunk. \\\n",
    "I indicates that the tag is inside a chunk. \\\n",
    "O indicates that a token belongs to no chunk.\n",
    "\n",
    "If we don't have a 'B' then it would not be possible to identify tokens if tokens appear next to each other.\n",
    "\n",
    "iob_words() and iob_sents() methods returns lists of three tuples of (word, pos, iob)\n",
    "\n",
    "IOB2 is a widely used format, same as the IOB format except that the B- tag is used in the\n",
    "beginning of every chunk (i.e. all chunks start with the B- tag).\n",
    "\n",
    "IOB2 format (with tagging unaffected by stop word filtering): \\\n",
    "\"Alex is going to Los Angeles in California.\" \n",
    "\n",
    "Alex B-PER \\\n",
    "is O \\\n",
    "going O \\\n",
    "to O \\\n",
    "Los B-LOC \\\n",
    "Angeles I-LOC \\\n",
    "in O \\\n",
    "California B-LOC "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d02cca6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f6ff2",
   "metadata": {},
   "source": [
    "**19. Write a tag pattern to cover noun phrases that contain gerunds, e.g. \"the/DT receiving/VBG end/NN\", \"assistant/NN managing/VBG editor/NN\". Add these patterns to the grammar, one per line. Test your work using some tagged sentences of your own devising.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c42733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6c44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "313c0934",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6f910a",
   "metadata": {},
   "source": [
    "**20. The bigram chunker scores about 90% accuracy. Study its errors and try to work out why it doesn't get 100% accuracy. Experiment with trigram chunking. Are you able to improve the performance anymore?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e385772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824569c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34d5abaa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5108a994",
   "metadata": {},
   "source": [
    "**21. Develop an NP chunker that converts POS-tagged text into a list of tuples, where each tuple consists of a verb followed by a sequence of noun phrases and prepositions, e.g. the little cat sat on the mat becomes ('sat', 'on', 'NP')...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf65384f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb1e6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badf2d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
